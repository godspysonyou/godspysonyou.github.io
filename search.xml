<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[神经网络]]></title>
    <url>%2F2018%2F02%2F03%2Fnnetwork%2Fnnetwork%2F</url>
    <content type="text"><![CDATA[简单神经网络参考《深度学习入门》，目的巩固和提升，以及便于日后的查询。 1 感知机1.1 什么是感知机感知机算法是美国学者Frank Rosenblatt在1957年提出来的。它很重要，是神经网络（深度学习）的起源。 感知机接受多个信号输出一个信号，如图1-1： 图1-1 用数学式子来表示：$$y = \begin{cases} 0 \quad(w_{1}x_{1}+w_{2}x_{2}\leq\theta) \1 \quad(w_{1}x_{1}+w_{2}x_{2}&gt;\theta)\end{cases} \qquad(1.1)$$ 感知机的国歌输入信号都有各自固有的权重，这些权重发挥着控制各个信号的作用。 1.2 从简单逻辑电路开始与门 1234567def AND(x1,x2): w1, w2, theta = 0.5, 0.5, 0.7 tmp = x1*w1 + x2*w2 if tmp &lt;= theta: return 0 elif tmp &gt; theta: return 1 导入权重和偏置的概念，来实现 12345678910import numpy as npdef AND(x1,x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1 与非门 123456789def NAND(x1,x2): x = np.array([x1, x2]) w = np.array([-0.5, -0.5]) b = 0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1 或门 123456789def OR(x1,x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.2 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1 1.3 感知机的局限如何线性区分以下空间 图1-2 可否用一条直线做出分割两种形状的空间 只有非线性才能做得到，如图1-3： 图1-3 使用非线性 1.4 多层感知机首先看看如何组合多中门电路来实现异或门 图1-4 逻辑门表示图1-5 组成异或门 异或门的代码实现 12345def XOR(x1,x2): s1 = NAND(x1, x2) s2 = OR(x1, x2) y = AND(s1, s2) return y 我们试着将上述逻辑门表示为感知机的形式，图1-6： 图1-6 多层感知机 这样一个“输入层-隐层-输出层”的多层感知机就可以解决非线性问题了。 1.5 精要 感知机（输入-[隐层]-输出） 感知机设定权重、偏置 单层感知机只能表示线性空间 多层感知机可以表示非线性空间 2 神经网络2.1 从感知机到神经网络 图2-1 简单神经网络构成 2.2 激活函数将神经网络表示成以下两个式子： $$a = b+w_{1}x_{1}+w_{2}x_{2} \qquad(2.1)$$$$y=h(a) \qquad(2.2)$$ 激活函数的作用决定如何激活输入信号的总和。 结构如图2-2所示： 图2-2 明确显示激活函数的计算过程 2.2.1 sigmoid函数神经网络中经常使用的一种激活函数是sigmoid函数，如下式：$$h(x)=\frac{1}{1+exp(-x)} \qquad(2.2)$$ sigmoid函数的实现 12def sigmoid(x): return 1 / (1 + np.exp(-x)) 看一下sigmoid函数的图像 1234567import matplotlib.pyplot as plt%matplotlib inlinex = np.arange(-5.0, 5.0, 0.1)y = sigmoid(x)plt.plot(x,y)plt.ylim(-0.1, 1.1) # 指定y轴的范围plt.show() 2.2.2 阶跃函数阶跃函数的实现 12345def step_function(x): if x &gt; 0: return 1 else: return 0 上面虽然实现简单、易于理解，但是参数x智能接受实数（浮点数）。也就是说不能接受numpy数组。为了使它更一般化，考虑下面的实现： 123def step_function(x): y = x &gt; 0 return y.astype(np.int) 看一下阶跃函数的图形 12345x = np.arange(-5.0, 5.0, 0.1)y = step_function(x)plt.plot(x, y)plt.ylim(-0.1, 1.1)plt.show() 2.2.3 sigmoid函数和阶跃函数的比较两者均为非线性函数，sigmoid函数是一条曲线，阶跃函数呈现阶梯形。不同的是，sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。平滑性（数学上[可导]可以推出[连续]可以推出[极限存在]，反之则不行）对神经网络的学习具有重要意义。 2.2.4 ReLU函数最近主要使用ReLU（Rectified Linear Unit）函数，修正线性单元，或者它的衍生版本。 ReLU可以表示为下式：$$h(x)=\begin{cases}x \quad(x&gt;0) \0 \quad(x\leq0)\end{cases} \qquad (2.3)$$ ReLU函数的实现： 12def relu(x): return np.maximum(0, x) ReLU函数的图像： 12345x = np.arange(-1.0, 1.0, 0.1)y = relu(x)plt.plot(x, y)plt.ylim(-0.1, 1.1)plt.show() 2.3 双隐层神经网络图像及其实现 图2-3 双隐层神经网络 123456789101112131415161718192021def init_network(): network = &#123;&#125; network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) network['b1'] = np.array([0.1, 0.2, 0.3]) network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) network['b2'] = np.array([0.1, 0.2]) network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) network['b3'] = np.array([0.1, 0.2]) return networkdef forward(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 return y 2.4 softmax函数分类问题中使用的softmax函数可以用下面：$$y_{k} = \frac{\exp(a_{k})}{\sum_{i=1}^{n}\exp(a_{i})} \qquad (2.4)$$ softmax可以使用如下实现： 123456def softmax(a): exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y 上面的实现虽然正确描述，但是在计算机的运算上会有问题————上溢出。因为需要进行指数运算，所以函数的值容易变得非常大。 数学上softmax函数的实现可以像式（2.5）一样改进：$$y_k = \frac{\exp(a_{k})}{\sum_{i=1}^{n}\exp(a_{i})} = \frac{C\exp(a_{k})}{C\sum_{i=1}^{n}\exp(a_{i})} \= \frac{\exp(a_{k}+\log C)}{\sum_{i=1}^{n}\exp(a_{i}+\log C)} \= \frac{\exp(a{k}+C^{‘})}{\sum_{i=1}^{n}\exp(a_{i}+C^{‘})} \qquad (2.5)$$ 上式说明softmax运算加上（减去）某一个常数并不会改变运算结果。这里我们去$C^{‘}$为输入信号中的最大值的负数 我们可以这样实现softmax： 1234567def softmax(a): c = np.max(a) exp_a = np.exp(a-c) # 溢出对策 sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y softmax函数的输出是0.0到1.0之间的实数，并且，softmax函数的输出值得总和是1。我们可以顺其自然的将输出与概率论联系起来。 2.5 简单手写数字识别应用介绍完基本神经网络结构之后，我们尝试解决实际问题。MINIST数据集是一个著名的手写数字图像集。其图像是$28\times28$的灰度图像（1通道）。数据集下载：https://pan.baidu.com/s/12is8fRSynrtoX4piAWfaeQ 图2-4 MINIST数据集示例 123456789import numpy as npmnist = np.load('mnist.npz')x_train, t_train, x_test, t_test = mnist['x_train'],mnist['y_train'],mnist['x_test'],mnist['y_test']# 输出各个数据的形状print(x_train.shape) # (60000, 784)print(t_train.shape) # (60000,) (60000, 28, 28) (60000,) 12print(np.max(x_train[0])) # 尚未normalizeprint(t_train[0]) # 尚未one-hot 255 5 normalize是将输入图像正规化为0.0~1.0的值（方便数学处理和神经网络适应）。 one-hot形式是将标签，如1，2表示成仅正确解标签为1，其余皆为0的数组，就像[0,1,0,0,0,0,0,0,0,0] 1234# mnist数据展示import matplotlib.pyplot as plt%matplotlib inlineplt.imshow(x_train[0], cmap=plt.cm.gray) &lt;matplotlib.image.AxesImage at 0x1152ccc18&gt; 下面我们对这个MNIST数据集实现神经网络的推理处理。我们可以根据数据集特性将这个网络结构设计为：输入层784个神经元，输出层为10个神经元。此外，使用两个隐层，第一个隐层有50个神经元，第2个隐层有100个神经元。权重文件在：https://pan.baidu.com/s/1KaH_a9fWTrsouYOCkIxKew 123456789101112131415161718192021222324def get_data(): x_train, y_train, x_test, y_test = mnist["x_train"],mnist["y_train"],mnist["x_test"],mnist["y_test"] x_test = x_test.reshape(10000,-1) return x_test, y_testdef init_network(): import pickle with open('sample_weight.pkl','rb') as f: # 已经训练完的权重文件，如训练将在后文提及 network = pickle.load(f) return networkdef predict(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = softmax(a3) return y 我们使用这三个函数来实现神经网络的推理处理，然后评价它的识别精度。 1234567891011x, t = get_data()x = x/255.0 # normalizenetwork = init_network()accuracy_cnt = 0for i in range(len(x)): y = predict(network, x[i]) p = np.argmax(y) # 获取概率最高的元素的索引 if p == t[i]: accuracy_cnt += 1print('Accuracy:'+str(float(accuracy_cnt)/len(x))) Accuracy:0.9352 2.6 批处理打包式的把数据输入。图像就如同纸币一样扎成一捆。 1234567891011121314x, t = get_data()x = x/255.0network = init_network()batch_size = 100 # 批数量accuracy_cnt = 0for i in range(0, len(x), batch_size): x_batch = x[i:i+batch_size] y_batch = predict(network, x_batch) p = np.argmax(y_batch, axis=1) # 这里的axis=1是每条数据[0，1，0，0，0]上比较，仔细理解下 accuracy_cnt += np.sum(p==t[i:i+batch_size]) print('Accuracy:'+str(float(accuracy_cnt) / len(x))) Accuracy:0.9352 2.7 精要 神经网络中激活函数使用平滑变化的sigmoid函数或ReLU及其变种等 NumPy多维数组可以高效实现神经网络 机器学习监督类学习大体上常分为回归和分类。机器学习和神经网络并不冲突 输出层的激活函数（分类问题常用softmax） 分类问题中输出层的神经元的数量设置为分类的类别数。 批处理，以批为单位，能够实现高速的运算 3 神经网络的“学习”这里的“学习”是指从训练数据中自动获取最优权重参数的过程。 3.1 从数据中学习同机器学习一样，神经网络也是数据驱动的。可以简化为一个最优化问题，以损失函数为基准，找出能使它的值达到最小的权重参数（方法是用梯度下降等）。 机器学习中，一般讲数据分为训练数据和测试数据。首先，使用训练数据进行学习，寻找最优的参数。为什么要用到测试数据？是为了正确评价模型的泛化能力。泛化能力是指处理未被观察过的数据的能力。 3.2 损失函数损失函数表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上的不拟合。 3.2.1 均方误差可以用作损失函数的数学函数有很多，其中比较常用的是均方误差(mean squared error)。均方误差如下式所示：$$E = \frac{1}{2}\sum_{k}(y_{k}-t_{k})^2 \qquad(3.1)$$这里$y_{k}$是神经网络的输出，$t_{k}$表示监督数据，$k$表示数据的维度。 均方误差的实现： 12def mean_squared_error(y, t): return 0.5 * np.sum((y-t)**2) 12345678# 设“2”为正确解t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]print(mean_squared_error(np.array(y1), np.array(t))) # 这个误差较低y2 = [0.1, 0.05, 0.0, 0.6, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]print(mean_squared_error(np.array(y2), np.array(t))) # 这个误差较高 0.09750000000000003 0.6974999999999999 均方误差显示与y1更为符合。 3.2.2 交叉熵误差此外，交叉熵误差（cross entropy error）也经常用作损失函数。如下：$$E = -\sum_{k}t_{k}\log y_{k} \qquad (3.2)$$ 这里，Log表示以e为底的自然对数($\log_{e}$)。$y_{k}$是神经网络的输出，$t_{k}$是正确解标签。并且，$t_{k}$中只有正确解标签的索引为1，其他均为0（one-hot表示）。因此，式(3.2)实际上只计算对应正确解标签的输出的自然对数。比如，假设正确解标签的索引是”2”，与之对应的神经网络的输出是0.6，则交叉熵误差是$-\log 0.6=0.51$；如果”2”对应的输出是0.1，那么交叉熵误差为$-\log 0.1=2.30$。也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。 这里有个小细节，交叉熵误差相对会比均方差（同等解标签对应数值下）来的更大一点，也就是说优化的空间更大（数值上）。 下面我们来用代码实现交叉熵误差。 123def cross_entropy_error(y, t): delta = 1e-7 return -np.sum(t * np.log(y + delta)) 这里加上一个微小值delta，这是因为当出现np.log(0)时，这个值将会负无限大-inf。 3.2.3 mini-batch学习前面介绍的损失函数的例子中考虑的都是针对单个数据的损失函数。如果要求所有（或批）训练数据的的损失函数的总和，以交叉熵误差为例，可以写成下式：$$E = -\frac{1}{N}\sum_{n}\sum_{k}t_{nk}\log y_{nk} \qquad (3.3)$$ mini-batch其实是计算单个数据和全部数据的一个折中。 mini-batch版交叉熵误差的实现： 12345678# 可以同时处理单个数据和批量数据，并且是标签是one-hot形式def cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum(t * np.log(y + 1e-7)) / batch_size 12345678# 标签是简单数字类型（像"2","3"这样的数字）def cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size 3.3 为何要设定损失函数精度不好吗，不把精度作为指标吗？ 从数学的导数来讲，在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。为了是损失函数值尽可能小，需要计算参数的导数（梯度），朝着梯度方向逐步更新参数。 如果现在用识别精度作为指标，这样绝大多数导数都是0，导致参数无法更新。根本无法学习。 3.4 数值微分梯度法使用梯度的信息决定参数优化前进的方向。 首先复习一下导数，某个瞬间的变化量：$$\frac{\text{d}f(x)}{\text{d}x}=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h} \qquad(3.4)$$ 用代码来实现一下： 123def numerical_diff(f, x): # 两个参数，”函数f“和”传给函数f的参数x“ h = 10e-50 return (f(x+h) - f(x)) / h 在上面的实现中，使用h=10e-50这个微小值（原本是想h趋近于0），会产生舍入误差，如下： 1np.float32(1e-50) 0.0 可以看到无法正确表示，直接变为0。所以将h改为10e-4。这是第一个改进点。 第二个改进的地方与函数f的差分有关，是从解析性上去解释的，如图3-1： 图3-1 真导数与数值微分的不同 所以以如下方式实现： 123def numerical_diff(f, x): h = 1e-4 return (f(x+h)-f(x-h)) / (2*h) 我们试着用上述数值微分对简单函数进行求导。看如下一个2次函数：$$y=0.01x^2+0.1x \qquad(3.5)$$ 用python来实现上述函数： 12def function_1(x): return 0.01*x**2 + 0.1*x 接下来，我们绘制一下这个函数的图像： 123456x = np.arange(0.0, 20.0, 0.1)y = function_1(x)plt.xlabel('x')plt.ylabel('f(x)')plt.plot(x,y)plt.show() 123# 计算一下在x=5和x=10处的导数print(numerical_diff(function_1, 5))print(numerical_diff(function_1, 10)) 0.1999999999990898 0.2999999999986347 真的导数分别为0.2和0.3，可以看到已经很接近了。 接下来继续深入一下偏导数，看如下的例子：$$f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2} \qquad(3.6)$$ 用Python来表示： 12def function_2(x): return x[0]**2 + x[1]**2 # 或者return np.sum(x**2) 我们来看一下这个函数的图像，如图3-2： 图3-2 $f(x_{0},x_{1})$的图像 接下来计算该函数梯度： 123456789101112131415161718def numerical_gradient(f, x): h = 1e-4 grad = np.zeros_like(x) # 生成x形状相同的属猪 for idx in range(x.size): tmp_val = x[idx] # f(x+h)的计算 x[idx] = tmp_val + h fxh1 = f(x) # f(x-h)的计算 x[idx] = tmp_val - h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) /(2*h) x[idx] = tmp_val # 还原值 return grad 1numerical_gradient(function_2, np.array([3.0, 4.0])) array([6., 8.]) 上面实际求得的是[6.000000000003781, 7.9999999999991189]，但是实际输出[6.,8.]，这是因为输出numpy数组时，会改成’易读‘形式。 我们来看一下$f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2}$的梯度图： 图3-3 $f(x_{0},x_{1})$的梯度 12]]></content>
      <categories>
        <category>machinelearning</category>
      </categories>
      <tags>
        <tag>machinelearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三角函数公式]]></title>
    <url>%2F2018%2F02%2F02%2Fsincostan%2F</url>
    <content type="text"><![CDATA[三角函数公式推导1 定义设角$\alpha$的终边与单位圆交于点$P(x,y)$，则有 $\sin\alpha=y,\cos\alpha=x$ $\tan\alpha=\frac{y}{x},\cot\alpha=\frac{x}{y}$ $\sec\alpha=\frac{1}{x},\csc\alpha=\frac{1}{y}$ 2 同角三家函数基本关系由上述定义可以得到三个倒数关系式： $\tan\alpha\cot\alpha=1$ $\sin\alpha\csc\alpha=1$ $\cos\alpha\sec\alpha=1$ 还可以得到如下商的关系： $\frac{\sin\alpha}{\cos\alpha}=\tan\alpha=\frac{\sec\alpha}{\csc\alpha}$ $\frac{\cos\alpha}{\sin\alpha}=\cot\alpha=\frac{\csc\alpha}{\sec\alpha}$ 结合勾股定理，得： $\sin^2\alpha+\cos^2\alpha=1$ 3 特殊值 4 诱导公式首先熟悉三个基本三函数$\sin和\cos和\tan$的性质 正弦函数是奇函数，最小正周期为2π，其导函数为余弦函数； 余弦函数是偶函数，最小正周期为2π，其导数为正弦函数的相反数； 正切函数是奇函数，最小正周期为π。 $\frac{k\pi}{2}+\alpha$ 奇变偶不变，符号看象限 5 基本公式推导由来： 平面上的两个单位向量，与x轴正向夹角分别为x和y，则这两个向量分别为$(\cos x,\sin x),(\cos y,\sin y)$。则这两个响亮的点积为$\cos x\cos y+\sin x\sin y$，而点积又可以表示为$1\times1\times\cos(x-y)$，于是得到如下公式： $\cos (x-y)=\cos x\cos y+\sin x\sin y \qquad(1)$ 6 和差角公式将(1)中的$y$用$-y$代入，即可得到 $\cos(x+y)=\cos x\cos y-\sin x\sin y \qquad(2)$ 将(1)中的$x$用$\frac{\pi}{2}-x$代入，利用诱导公式，可以得到正弦函数的和差角公式： $\sin(x+y)=\cos y\sin x+\cos x\sin y \qquad(3)$ (3)式中的$y$用$-y$代替，有 $\sin(x-y)=\cos y\sin x-\cos x\sin y \qquad(4)$ (3)/(2),(4)/(1),得到正切函数的和差角公式： $\tan(x+y)=\frac{\tan x+\tan y}{1-\tan x\tan y} \qquad(5)$ $\tan(x-y)=\frac{\tan x-\tan y}{1+\tan x\tan y} \qquad(6)$ 7 倍角公式和半角公式在标题6中提到的式子，令$x=y$，很容易得到倍角公式和半角公式： $\sin 2x=2\sin x\cos x \qquad(7)$ $\cos 2x=\cos^2 x-\sin^2 x \qquad(8)$ $\tan 2x=\frac{2\tan x}{1-\tan^2 x} \qquad(9)$ 注意到(8)式中，由平方关系又可以改写成$\cos 2x=2\cos^2 x-1或\cos 2x=1-\sin^2 x$。所以可以直接得到半角公式： $\sin^2\frac{x}{2}=\frac{1-\cos x}{2} \qquad(10)$ $\cos^2\frac{x}{2}=\frac{1+\cos x}{2} \qquad(11)$ $\tan^2\frac{x}{2}=\frac{1-\cos x}{1+\cos x} \qquad(12)$ 8 积化和差与和差化积(3)+(4)得： $\sin x\cos y=\frac{1}{2}[\sin(x+y)+sin(x-y)] \qquad(13)$ (3)-(4)得： $\cos x\sin y=\frac{1}{2}[\sin(x+y)-\sin(x-y)] \qquad(14)$ (1)+(2)得： $\cos x\cos y=\frac{1}{2}[\cos(x+y)+\cos(x-y)] \qquad(15)$ (1)-(2)得： $\sin x\sin y=\frac{1}{2}[\cos(x-y)-\cos(x+y)] \qquad(16)$ 然后再上式中，令$u=x+y,v=x-y$,此时$x=\frac{u+v}{2},y=\frac{u-v}{2}$,得到和差化积公式： $\sin u+\sin v=2\sin\frac{u+v}{2}\cos\frac{u-v}{2} \qquad(17)$ $\sin u-\sin v=2\cos\frac{u+v}{2}\sin\frac{u-v}{2} \qquad(18)$ $\cos u+\cos v=2\cos\frac{u+v}{2}\cos\frac{u-v}{2} \qquad(19)$ $\cos u-\cos v=-2\sin\frac{u+v}{2}\sin\frac{u-v}{2} \qquad(20)$ 9 万能公式万能公式是将$\sin x,\cos x和\tan x$均用$\tan\frac{x}{2}$表示。因为$\tan\frac{x}{2}$的值域为整个实数空间，所以方便考察 余弦函数的万能公式： $\cos x=\frac{1-\tan^2\frac{x}{2}}{1+\tan^2\frac{x}{2}} \qquad(21)$ $\sin x=\frac{2\tan x}{1-\tan^2 x} \qquad(22)$]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F02%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
